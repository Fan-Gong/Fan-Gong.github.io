<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Fan Gong's Blog</title>

    <!-- Bootstrap core CSS -->
    <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="shortcut icon" href="../../img/title.png" type="image/png">

    <!-- Custom fonts for this template -->
    <link href="../../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="../../css/clean-blog.min.css" rel="stylesheet">

    <script type="text/x-mathjax-config"> MathJax.Hub.Config({
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            }});
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
    </script>

    <style>
      table {
          font-family: arial, sans-serif;
          border-collapse: collapse;
          width: 100%;
      }

      td, th {
          border: 1px solid #dddddd;
          text-align: left;
          padding: 8px;
      }

      tr:nth-child(even) {
          background-color: #dddddd;
      }
    </style>

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="../../index.html">Fan Gong's Blog</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="../../index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../about.html">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../article.html">Article</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('background5.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Recurrent Neural Networks (Including LSTM and GRU)</h1>
              <h2 class="subheading">Neural Network Learning notes (5)</h2>
              <span class="meta">Posted by
                <a href="../../about.html">Fan Gong</a>
                on Apr 21, 2019</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">


            <p>RNN was designed for dealing with sequence data problem. For the next several articles, let&#39;s talk about that and finish the reviewing of deep learning basic knowledge.</p>

            <h2 id="toc_1">1. Intro to RNN</h2>

            <h3 id="toc_2">1.1 Why we need it</h3>

            <ul>
            <li><p><strong>Inputs and outputs length can be different</strong>: Remember CNN and RNN can only accept a fixed-size vector as input and produce a fixed-size output. But RNN can have different length of input and output:
            <div style="text-align: center;">
                    <img class="img-fluid" src="5.2.jpg" alt="" align='middle'> <br>
                    <a href = "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"> Image is from karpathy</a>
            </div></p></li>
            <li><p><strong>Recurrent nets allow us to operate over sequences of vectors</strong>. In another word, it will share features learned across different positions of the input and allow information to flow from one step to the next.</p></li>
            </ul>

            <h3 id="toc_3">1.2 The problem it can solve</h3>

            <p>RNN has been widely used recently due to the large volume of sequence data in real life. In general, here are the fields that RNN is used frequently: </p>

            <ul>
            <li>NLP: including sentiment analysis, language model, machine translation; speech recognition; image captioning<br></li>
            <li>Time-series processing including stock price prediction<br></li>
            </ul>

            <h2 id="toc_4">2. RNN</h2>

            <h3 id="toc_5">2.1 RNN Structure</h3>

            <p>In order to solve the problem that normal FNN cannot remember the order and use previous information, we simply <strong>add a &#39;time axis&#39; in the network structure that can pass prior information forward</strong>. That’s essentially what a recurrent neural network does:
            <div style="text-align: center;">
                        <img class="img-fluid" src="5.1.jpg" alt="" align='middle'> <br>
                        <a href = "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"> Image is from colah&#39;s blog</a>
            </div>
            Where each time we only pass one feature of a sample and move it forward with order. </p>

            <p>Mathematically:
            \[A_t = g(\omega_{aa}A_{t-1} +\omega_{ax}X_{t} + b_a)\]
            Which contains this layer&#39;s hidden unit information. We see it uses not only the new input \(X_{t}\) but also last hidden unit information \(A_{t-1}\). Here activation function \(g(x)\) normally will be tanh or relu. </p>

            <p>Then if we output this layer&#39;s information:
            \[h_t = g(\omega_{ya} A_{t} + b_y)\]
            Here normally we will choose sigmoid function as \(g(x)\) </p>

            <h3 id="toc_6">2.2 Limitation of RNN</h3>

            <h4 id="toc_7">2.2.1 Vanishing Gradient</h4>

            <p>RNN unit is good for sequence data predictions but suffer from short-term memory. I found a very good illustration from <a href = "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9"> Michael Nguyen&#39;s blog </a>: </p>

            <div style="text-align: center;">
                        <img class="img-fluid" src="5.3.jpg" alt="" align='middle'> <br>
             </div>

            <p>Suppose we have already trained a RNN with large corpus, and we want it to work as a chatbot that answers the question people has asked. Technically we will take the final output and pass it to the feed-forward layer to classify an intent. We can tell from the last recurrent unit&#39;s distribution that it hardly uses information from words &#39;what&#39; and &#39;time&#39;, and mainly uses information &#39;is is ?&#39; to make the prediction. Thus it is very natural to have a bad prediction output. That&#39;s what we called short-term memory. Why that happens? -- vanishing gradient problem again.</p>

            <blockquote class="blockquote"> When doing back propagation, each node in a layer calculates it’s gradient with respect to the effects of the gradients, in the layer before it. So if the adjustments to the layers before it is small, then adjustments to the current layer will be even smaller. That causes gradients to exponentially shrink as it back propagates down. The earlier layers fail to do any learning as the internal weights are barely being adjusted due to extremely small gradients. And that’s the vanishing gradient problem.
            </blockquote>

            <p>Actually we can see that the lots of improvement in deep learning field is due to the success of solving the vanishing gradient problem. For RNN, LSTM and GRU were created as a method to mitigate short-term memory problem. </p>

            <h4 id="toc_8">2.2.2 One direction information</h4>

            <p>Another limitation of RNN is that the prediction at a certain time only uses inputs earlier in the sequence but not information later in the sequence. To solve that problem, we will use bi-directional RNN.  </p>

            <h2 id="toc_9">3. LSTM</h2>

            <p>Both LSTM and GRU were created to solve the gradient vanishing problem by using a mechanism called gates. <strong>Gates are normal neural networks that regulate the flow of information being passed from one time step to the next</strong>. Let&#39;s look at LSTM step by step. </p>

            <h3 id="toc_10">3.1 <strong>Cell State</strong></h3>

            <p>Cell State is the key to LSTMs. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged,<strong>which is the reason it can refrain from vanishing gradient.</strong> I think the logic is kind of similar to Residual CNN: we make this layer similar with previous one if there is no learning, which indirectly shallows the network and helps with vanishing gradient.
            <div style="text-align: center;">
                        <img class="img-fluid" src="5.4.jpg" alt="" align='middle'> <br><a href = "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"> Image is from colah&#39;s blog</a>
             </div></p>

            <h3 id="toc_11">3.2 Forget Gate</h3>

            <p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state, and we use sigmoid function to decide that: (sigmoid function output value from 0-1 for each feature, after element-wise multiplication, we will get the final output)
            \[f_t=\sigma(\omega_f[h_{t-1},x_t]+b_f)\]
            <div style="text-align: center;">
                        <img class="img-fluid" src="5.5.jpg" alt="" align='middle'> <br><a href = "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"> Image is from colah&#39;s blog</a>
             </div></p>

            <h3 id="toc_12">3.3 Update Gate</h3>

            <p>The next step is to decide what information we want to keep in the cell state. Same with forget gate, we use sigmoid function to decide. After that it will multiply with the new candidate value \(\tilde{C_t}\).
            \[\tilde{C_t} = tanh(\omega_c[h_{t-1},x_t]+b_i)\]
            \[i_t = \sigma(\omega_i[h_{t-1},x_t]+b_i)\]
            <div style="text-align: center;">
                        <img class="img-fluid" src="5.6.jpg" alt="" align='middle'> <br><a href = "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"> Image is from colah&#39;s blog</a>
             </div></p>

            <p>Then it’s now the time to update the old cell state, \(C_{t-1}\), into the new cell state \(C_t\) by using forget and update gate&#39;s information:
            \[C_{t} = f_t * C_{t-1} + i_t * \tilde{C_t}\]</p>

            <h3 id="toc_13">3.4 Output Gate</h3>

            <p>Finally we need to decide what we are going to output in the cell state by using output gate:
            \[o_t = \sigma(\omega_o[h_{t-1},x_t]+b_o)\]
            \[h_{t} = o_t * tanh(C_t)\]</p>

            <h2 id="toc_14">4. GRU</h2>

            <p>GRU is the simplified version of LSTM, it combines the forget and update gate together and also without using the memory unit(the cell state). Thus it is computationally faster. Mathematically:
            \[z_t = \sigma(\omega_z[h_{t-1},x_t]+b_z)\]
            Which works as update gate similar to what we have in LSTM
            \[r_t = \sigma(\omega_r[h_{t-1},x_t]+b_r)\]
            Which called reset gate that decides how much past information to forget.
            \[\tilde{h_t} = tanh(\omega[r_t * h_{t-1},x_t]+b)\]
            \[h_t = z_t * \tilde{h_t} + (1-z_t)* h_{t-1} \]
            <div style="text-align: center;">
                        <img class="img-fluid" src="5.7.jpg" alt="" align='middle'> <br><a href = "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"> Image is from colah&#39;s blog</a>
             </div></p>

            <p><blockquote class="blockquote"> Reference: <br></p>

            <p><a href="https://www.coursera.org/specializations/deep-learningaction=enroll&authMode=login&specialization=deep-learning"> DeepLearning.ai</a><br>
             <a href = "https://www.youtube.com/watch?v=8HyCNIVRbSU">https://www.youtube.com/watch?v=8HyCNIVRbSU</a>
             <a href = "http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>
            <a href = "http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>

            <p></blockquote class="blockquote"></p>






          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/Fan93fine">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.facebook.com/profile.php?id=100012777241298">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/Fan-Gong">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Fan Gong 2019</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
