<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Fan Gong's Blog</title>

    <!-- Bootstrap core CSS -->
    <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="shortcut icon" href="../../img/title.png" type="image/png">

    <!-- Custom fonts for this template -->
    <link href="../../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="../../css/clean-blog.min.css" rel="stylesheet">

    <script type="text/x-mathjax-config"> MathJax.Hub.Config({
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            }});
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
    </script>

    <style>
      table {
          font-family: arial, sans-serif;
          border-collapse: collapse;
          width: 100%;
      }

      td, th {
          border: 1px solid #dddddd;
          text-align: left;
          padding: 8px;
      }

      tr:nth-child(even) {
          background-color: #dddddd;
      }
    </style>

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="../../index.html">Fan Gong's Blog</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="../../index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../about.html">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../article.html">Article</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('background.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>How to Improve Neural Networks (1)</h1>
              <h2 class="subheading">Neural Network Learning notes (2)</h2>
              <span class="meta">Posted by
                <a href="../../about.html">Fan Gong</a>
                on March 21, 2019</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">

            <p>To improve the performance of our neural network, we mainly focus on these two aspects:</p>

            <ol>
            <li>Accuracy - That is very easy to understand. To make it more precise, we want to improve the test accuracy</li>
            <li>Speed - the big difference between normal ML algorithm and Neural Network is that DNN always needs long time to train the model. In that case, efficiency or running time is also a aspect to improve</li>
            </ol>

            <p>I divided this topic into two blogs. The first blog talks about the basic data pre-processing methods and why it will work; Different ways to overcome the overfitting problem. The second blog will mainly focus on talking about different optimization methods. </p>

            <h2 id="toc_1">1. Data Preprocessing</h2>

            <h3 id="toc_2">1.1 Data Normalization</h3>

            <p>Data normalization means normalize your data with same \(\mu\) and \(\sigma\), in most cases we will subtract the mean and divide its std for each feature. After normalization each feature will have \(\mu =0\) and \(\sigma =1\) </p>

            <ul>
            <li><strong>Why we need it?</strong>: This course gives very good graph explanation:
            <div style="text-align: center;">
                    <img class="img-fluid" src="1.jpg" alt="" align='middle'>
            </div>
            <strong>It prevents the cost function to be an elongated bowl, which makes gradient descent inefficient</strong></li>
            </ul>

            <h3 id="toc_3">1.2 Weight Initialization</h3>

            <p>The necessity of weight initialization is due to the problem called <strong>Vanishing/Exploding Gradients</strong></p>

            <p>Suppose all initial weights of \(\omega &gt; 1\), and we use a linear activation function, then the gradient will be larger and larger which makes it difficult to find the local minimum. Same cases if \(\omega &lt; 1\), the gradient will become smaller and smaller and almost stop the weights update.</p>

            <p>In that case we need to random initialize the weight (symmetry breaking).  Normally, we will sample weight from standard normal distribution but with \(Var(\omega_i) = \frac{1}{m}\) where m is the number of features of last layer. <strong>Intuition here is that as the feature number increasing, we decrease the weights --&gt; slight regularization</strong></p>

            <h2 id="toc_4">2. Regularization</h2>

            <p>Bias and Variance trade-off in DNN is much less than normal ML. In a word, we always reduce bias without hurting variance by using bigger network.</p>

            <h3 id="toc_5">2.1 Regularization</h3>

            <p>Just like normal ML algorithm, we can have L1 or L2 regularization to prevent overfitting. <strong>Intuition behind is to decay the weight that makes the network simple</strong></p>

            <p>\[J(\omega^{[1]},b^{[1]},...) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y^{i}},y^{I})+\frac{\lambda}{2m}\sum_{l=1}^{[l]}||\omega^{[l]}||^2_{2}\]</p>

            <h3 id="toc_6">2.2 Dropout</h3>

            <p>The way dropout doing is to only keep a node active with some probability. In another word, everything training we will randomly throw out some neural. The common technique we used is called inverted dropout. </p>

            <p><strong>Inverted Dropout</strong>: what it does is not only randomly zero out some proportion units in a layer, but also try to keep the expectation value the same by doing scaling up:
            \[\frac{g(z^{[l]})}{p(dropout)}\]
            In that case dropout will not be used in the test time and the expectation value of each layer is the same with training time. </p>

            <p><strong>Intuition</strong>: The model cannot rely one any one feature since as the Gradient Descent goes, any feature may be dropped out ==&gt; similar to shrink some weights to zero.</p>

            <p>But the drawback is our cost function is changing every time. </p>

            <p>Let&#39;s look at other ways to improve the NN in the next article. </p>

            <p><blockquote class="blockquote"> Reference: <br>
             <a href="https://www.coursera.org/specializations/deep-learningaction=enroll&authMode=login&specialization=deep-learning"> DeepLearning.ai</a><br>
             <a href="https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb"> https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb</a><br>
             <a href="http://cs231n.github.io/neural-networks-2/#reg"> http://cs231n.github.io/neural-networks-2/#reg</a><br>
             </blockquote class="blockquote"></p>







          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/Fan93fine">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.facebook.com/profile.php?id=100012777241298">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/Fan-Gong">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Fan Gong 2018</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
