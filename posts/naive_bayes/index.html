<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Fan Gong's Blog</title>

    <!-- Bootstrap core CSS -->
    <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="shortcut icon" href="../../img/title.png" type="image/png">

    <!-- Custom fonts for this template -->
    <link href="../../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="../../css/clean-blog.min.css" rel="stylesheet">

    <script type="text/x-mathjax-config"> MathJax.Hub.Config({
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            }});
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
    </script>

    <style>
      table {
          font-family: arial, sans-serif;
          border-collapse: collapse;
          width: 100%;
      }

      td, th {
          border: 1px solid #dddddd;
          text-align: left;
          padding: 8px;
      }

      tr:nth-child(even) {
          background-color: #dddddd;
      }
    </style>

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="../../index.html">Fan Gong's Blog</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="../../index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../about.html">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../article.html">Article</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="../../contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('../../img/bayes.png')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Naive Bayes Classifier</h1>
              <h2 class="subheading">Generative Classification Model</h2>
              <span class="meta">Posted by
                <a href="../../about.index">Fan Gong</a>
                on January 18, 2018</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">

            <p>Last time we talked about logistic regression, but I did not give a very overview idea about what the classification problem is and how can we classify different classification methods. So let us talk them this time!</p>
            <p>Most classification models can be categorized as generative or discriminative. What is the difference?</p>

            <p><strong>Generative</strong> models focus on modeling the $P(X,Y)$ and then use $\hat{Y(X)}$ as a classifier where:<br />
            $$\hat{Y(X)} = \arg \max_{Y} P(Y|X) = \arg \max \frac{P(X|Y)P(Y)}{\sum_i P(X|Y_i)P(Y_i)} = \arg \max_Y P(X|Y)P(Y)$$<br />
            Example includes <em>linear discriminant analysis (LDA)</em>, <em>quadratic discriminant analysis (QDA)</em> and <em>Naive Bayes</em></p>
            <p><strong>Discriminative</strong> methods focus on modeling $P(Y|X)$ directly. So these methods aim to minimize the loss directly without making any assumptions about $P(X, Y)$ or $P(Y|X)$. Most of the classification methods belong to this category such as the <em>logistic regression</em> we talked before, and also <em>SVMs</em>, <em>knn</em> and <em>classification trees</em>.</p>

            <hr>

            <h2 id="introduction" class="subheading">1. Introduction</h2>
            <p>I will cover three generative classification models in the next several posts: <strong>LDA</strong>, <strong>QDA</strong> and <strong>Naive Bayes</strong>. Here are their basic differences:</p>
            <ol>
            <li>LDA and QDA assume Gaussian density for $P(X|Y)$, but LDA needs the variance for each class to be the same, and QDA does not.</li>
            <li>Naive Bayes assumes $P(X|Y) = \prod_j P(X_j|Y)$</li>
            </ol>
            <br>
            <h2 id="bayes-classifier" class="subheading">2. Bayes Classifier</h2>
            <p>Before we talk about Naive Bayes, let us review what is bayes classifier first.</p>
            <p>Suppose we know $P(Y|X)$, then given an input, we could predict the response:<br />
            $$\hat{y_0} = \arg\max_y P(Y=y|X=x_0)$$<br />
            and it is the best classifier under the expected 0-1 loss, Why? Here is the brief proof: (I think this part is very necessary to know)</p>
            <blockquote>
              <p>The 0-1 binary loss function has the following form:</p>

              <p>$$ l_\boldsymbol{x}(\hat s, s^*) = 1 - \delta_{\hat ss^*} =
             \begin{cases} 1 &amp; \text{if} \quad \hat s \ne s^* \\ 0 &amp;
             \text{otherwise} \end{cases} $$</p>

              <p> where $\delta$ is the Kronecker Delta function. (...) the expected loss is:</p>

              <p>$$ \begin{align} \mathcal{L}_\boldsymbol{x}(\hat s) &amp;= \sum_{s^*}
             l_\boldsymbol{x}(\hat s, s^*) \; P(s = s^* \mid \boldsymbol{x}) \\ &amp;=
             \sum_{s^*} (1 - \delta_{\hat ss^*}) \; P(s = s^* \mid \boldsymbol{x})
             \\ &amp;= \sum_{s^*} P(s = s^* \mid \boldsymbol{x}) - \sum_{s^*} \delta_{\hat ss^*} P(s = s^* \mid \boldsymbol{x}) \\ &amp;= 1 - P(s = s^*
             \mid \boldsymbol{x}) \end{align} $$</p>
            </blockquote>
            <span class="caption text-muted">Use for reference from <a href = "https://stats.stackexchange.com/questions/296014/why-is-the-naive-bayes-classifier-optimal-for-0-1-loss">Cross Validated</a></span>
            <p>So we could see, <strong>minimize the expected 0-1 loss equals to maximize the $P(s=s^*|\boldsymbol{x})$</strong> and <strong>if we want to find $\arg \min_s \mathcal{L}_\boldsymbol{x}(\hat s)$, it equals to find $\arg \max_s P(s = s^*|\boldsymbol{x})$.</strong></p>
            <p>After knowing the relationship between 0-1 loss and bayes classifier, we then could have a much clear understanding of naive bayes.</p>
            <p>Finally, Since these &quot;true&quot; probabilities are essentially never known, the Bayes Classifier is more a theoretical concept and not something that we can actually use in practice.</p>
            <h2 id="naive-bayes-classifier" class="subheading">3. Naive Bayes Classifier</h2>
            <p>Naive Bayes Classifier could be seen as a simple case of Bayes Classifier. Since Bayes Classifier is hard to calculate in practical, Naive Bayes make some assumptions to make it computable: it assumes that the features are independent conditional on the class $Y$.</p>
            <p>It is a strong and generally unrealistic assumption but naive Bayes still often works very well in practice.</p>
            <h3 id="model-construction">3.1 Model Construction</h3>
            <p>We start from the basic generative model:<br />
            $$\hat{Y(X)} = \arg \max_{Y} P(Y|X) = \arg \max \frac{P(X|Y)P(Y)}{\sum_i P(X|Y_i)P(Y_i)} = \arg \max_Y P(X|Y)P(Y)$$</p>
            <p>And for naive bayes model, we have (suppose we have p features):<br />
            $$\hat{Y} = \arg \max_Y P(X|Y)P(Y) = \arg \max_Y P(x_1,...,x_p|Y)P(Y)= \arg \max_Y \prod_{i=1}^p P(x_i|Y)P(Y)$$</p>
            <ul>
            <li>The distribution $P(Y)$ is easy to estimate from training data: $$ P(y) = \frac{\# \ of \ observations \ in \ class\ y}{\# \ of\ observations}$$ </li>
            <li>The class conditionals $P(x_i|Y=y)$ usually requires a distribution assumption. When dealing with continuous data, a typical assumption is normal distribution; for discrete data, we always use multinomial distribution. Then we will use MLE to estimate the parameters.</li>
            </ul>
            <h3 id="model-implementation">3.2 Model Implementation</h3>
            <p>This time I compare my Naive Bayes classifier's result with <code>sklearn.naive_bayes.GaussianNB</code> 's result, and they both have an 87% accuracy, which is pretty good. Here is my code for Naive Bayes Classifier from scratch:</p>
            <p><a href="./Naive_Bayes_Classifier.html">Jupyter Notebook For Naive Bayes Classifier </a></p>
            <p>Let us talk other generative classification methods next time!</p>












          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/Fan93fine">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.facebook.com/profile.php?id=100012777241298">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/Fan-Gong">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Fan Gong 2018</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
